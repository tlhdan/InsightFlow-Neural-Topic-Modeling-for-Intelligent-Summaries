import os
import numpy as np
import gensim
import pickle
import torch
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from gensim.corpora import Dictionary
from gensim.models import TfidfModel
from collections import Counter
from tokenization import *

class DocDataset(Dataset):
    def __init__(self, taskname, txtPath=None, lang="zh", tokenizer=None, stopwords=None,
             no_below=5, no_above=0.1, hasLabel=False, rebuild=False, use_tfidf=False):

        # XÃ¡c Ä‘á»‹nh thÆ° má»¥c dá»¯ liá»‡u
        self.base_dir = "/content/InsightFlow-Neural-Topic-Modeling-for-Intelligent-Summaries/data"  
        os.makedirs(self.base_dir, exist_ok=True)

        # XÃ¡c Ä‘á»‹nh Ä‘Æ°á»ng dáº«n tá»‡p dá»¯ liá»‡u
        if txtPath is None:
            txtPath = os.path.join(self.base_dir, f'{taskname}_lines.txt')

        tmpDir = os.path.join(self.base_dir, taskname)

        # Kiá»ƒm tra xem file cÃ³ tá»“n táº¡i khÃ´ng
        if not os.path.exists(txtPath):
            raise FileNotFoundError(f"âŒ KhÃ´ng tÃ¬m tháº¥y tá»‡p: {txtPath}. HÃ£y kiá»ƒm tra Ä‘Æ°á»ng dáº«n!")

        print(f"ðŸ“‚ Äang má»Ÿ tá»‡p: {txtPath}")

        # Äá»c dá»¯ liá»‡u tá»« tá»‡p
        with open(txtPath, 'r', encoding='utf-8') as f:
            self.txtLines = [line.strip() for line in f]

        self.dictionary = None
        self.bows, self.docs = None, None
        self.use_tfidf = use_tfidf
        self.tfidf, self.tfidf_model = None, None

        if not os.path.exists(tmpDir):
            os.makedirs(tmpDir)

        if not rebuild and os.path.exists(os.path.join(tmpDir, 'corpus.mm')):
            self.bows = gensim.corpora.MmCorpus(os.path.join(tmpDir, 'corpus.mm'))
            if self.use_tfidf:
                self.tfidf = gensim.corpora.MmCorpus(os.path.join(tmpDir, 'tfidf.mm'))
            self.dictionary = Dictionary.load_from_text(os.path.join(tmpDir, 'dict.txt'))
            self.docs = pickle.load(open(os.path.join(tmpDir, 'docs.pkl'), 'rb'))
        else:
            if stopwords is None:
                stopwords_path = os.path.join(self.base_dir, 'stopwords.txt')
                if os.path.exists(stopwords_path):
                    stopwords = set([l.strip() for l in open(stopwords_path, 'r', encoding='utf-8')])
                else:
                    stopwords = set()

            print('Tokenizing ...')
            if tokenizer is None:
                tokenizer = globals()[LANG_CLS[lang]](stopwords=stopwords)
            self.docs = tokenizer.tokenize(self.txtLines)
            self.docs = [doc for doc in self.docs if doc]

            self.dictionary = Dictionary(self.docs)
            self.dictionary.filter_extremes(no_below=no_below, no_above=no_above)
            self.dictionary.compactify()

            self.bows = [self.dictionary.doc2bow(doc) for doc in self.docs if doc]
            if self.use_tfidf:
                self.tfidf_model = TfidfModel(self.bows)
                self.tfidf = [self.tfidf_model[bow] for bow in self.bows]

            gensim.corpora.MmCorpus.serialize(os.path.join(tmpDir, 'corpus.mm'), self.bows)
            self.dictionary.save_as_text(os.path.join(tmpDir, 'dict.txt'))
            pickle.dump(self.docs, open(os.path.join(tmpDir, 'docs.pkl'), 'wb'))

        self.vocabsize = len(self.dictionary)
        self.numDocs = len(self.bows)
        print(f'âœ… ÄÃ£ xá»­ lÃ½ {self.numDocs} tÃ i liá»‡u.')


    def __getitem__(self,idx):
        bow = torch.zeros(self.vocabsize)
        if self.use_tfidf:
            item = list(zip(*self.tfidf[idx]))
        else:
            item = list(zip(*self.bows[idx])) # bow = [[token_id1,token_id2,...],[freq1,freq2,...]]
        bow[list(item[0])] = torch.tensor(list(item[1])).float()
        txt = self.docs[idx]
        return txt,bow

    def __len__(self):
        return self.numDocs

    def collate_fn(self,batch_data):
        texts,bows = list(zip(*batch_data))
        return texts,torch.stack(bows,dim=0)

    def __iter__(self):
        for doc in self.docs:
            yield doc

    def show_dfs_topk(self,topk=20):
        ndoc = len(self.docs)
        # print(self.dictionary.id2token[k])
        # dfs_topk = sorted([(self.dictionary.id2token[k],fq) for k,fq in self.dictionary.dfs.items()],key=lambda x: x[1],reverse=True)[:topk]
        dfs_topk = sorted([(self.dictionary.id2token.get(k, f"UNK_{k}"), fq) for k, fq in self.dictionary.dfs.items()],
                   key=lambda x: x[1], reverse=True)[:topk]
        for i,(word,freq) in enumerate(dfs_topk):
            print(f'{i+1}:{word} --> {freq}/{ndoc} = {(1.0*freq/ndoc):>.13f}')
        return dfs_topk

    def show_cfs_topk(self,topk=20):
        ntokens = sum([v for k,v in self.dictionary.cfs.items()])
        cfs_topk = sorted([(self.dictionary.id2token[k],fq) for k,fq in self.dictionary.cfs.items()],key=lambda x: x[1],reverse=True)[:topk]
        for i,(word,freq) in enumerate(cfs_topk):
            print(f'{i+1}:{word} --> {freq}/{ntokens} = {(1.0*freq/ntokens):>.13f}')

    def topk_dfs(self,topk=20):
        ndoc = len(self.docs)
        dfs_topk = self.show_dfs_topk(topk=topk)
        return 1.0*dfs_topk[-1][-1]/ndoc
if __name__ == '__main__':
    # dataset = DocDataset('EMNLP2020', rebuild=True)
    # dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=0, collate_fn=dataset.collate_fn)
    # data_iterator = iter(dataloader)
    # print(next(data_iterator))
    # print('Top 20 frequent words:')
    # dataset.show_dfs_topk()

    docSet = DocDataset('EMNLP2020',rebuild=True)
    dataloader = DataLoader(docSet,batch_size=64,shuffle=True ,num_workers=2,collate_fn=docSet.collate_fn)
    print('docSet.docs[10]:',docSet.docs[10])
    print(iter)
    # del iter
    print(next(iter(dataloader)))
    print('The top 20 tokens in document frequency:')
    docSet.show_dfs_topk()
    print('The top 20 tokens in collections frequency:')
    input("Press any key ...")
    # docSet.show_cfs_topk()
    # input("Press any key ...")
    for doc in docSet:
        print(doc)
        break
    print(docSet.topk_dfs(20))